#!/usr/bin/env python3
"""
MoonBit å‘¨æŠ¥ä»“åº“æ”¶é›†å™¨ v3.0
ä½¿ç”¨ DeepSeek-V3 AI åˆ†ç±»ï¼Œé‡é‡çº§æ•°æ®æŠ“å–
"""

import sys
import re
import logging
import subprocess
from datetime import datetime
from pathlib import Path

from config import (
    WEEKLY_DIR,
    OUTPUT_DIR,
    OUTPUT_FILENAME_FORMAT,
    get_weekly_number
)
from fetcher import GitHubFetcher
from classifier import RepoClassifier
from formatter import MarkdownFormatter
import json

# ============ æ—¥å¿—é…ç½® ============

def setup_logging():
    """è®¾ç½®æ—¥å¿—ï¼ˆä»…ç»ˆç«¯è¾“å‡ºï¼‰"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(message)s',
        handlers=[
            logging.StreamHandler()
        ]
    )

# ============ æ—¥æœŸå¤„ç† ============

def auto_detect_date():
    """ä»æœ€æ–°å‘¨æŠ¥æå–æˆªæ­¢æ—¥æœŸ"""
    try:
        weekly_files = list(WEEKLY_DIR.glob("weekly*.md"))
        if not weekly_files:
            return None
        
        # æŒ‰æ•°å­—æ’åº
        latest = max(weekly_files, 
                     key=lambda p: int(re.search(r'\d+', p.stem).group()))
        
        # è¯»å– frontmatter
        content = latest.read_text(encoding='utf-8')
        
        # æå–ï¼štitle: Weekly14 ç¤¾åŒºå‘¨æŠ¥ 2025/9/22 ~ 2025/10/8
        match = re.search(r'~\s*(\d{4}/\d{1,2}/\d{1,2})', content)
        
        if match:
            date_str = match.group(1)
            parts = date_str.split('/')
            return f"{parts[0]}-{parts[1]:0>2}-{parts[2]:0>2}"
        
    except Exception as e:
        logging.warning(f"è‡ªåŠ¨æ£€æµ‹æ—¥æœŸå¤±è´¥ï¼š{e}")
    
    return None

def get_search_date():
    """è·å–æœç´¢æ—¥æœŸï¼ˆæ™ºèƒ½ï¼šè‡ªåŠ¨æ£€æµ‹æˆ–æ‰‹åŠ¨è¾“å…¥ï¼‰"""
    # æ”¯æŒå‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1:
        date_arg = sys.argv[1]
        if re.match(r'\d{4}-\d{2}-\d{2}', date_arg):
            return date_arg
        else:
            print(f"âŒ æ—¥æœŸæ ¼å¼é”™è¯¯ï¼š{date_arg}")
            sys.exit(1)
    
    auto_date = auto_detect_date()
    
    if auto_date:
        print(f"ğŸ“… æ£€æµ‹åˆ°ä¸Šæ¬¡å‘¨æŠ¥æ—¥æœŸï¼š{auto_date}")
        try:
            confirm = input("ç¡®è®¤ä½¿ç”¨ï¼Ÿ(y/n): ").strip().lower()
            if confirm == 'y':
                return auto_date
        except EOFError:
            # éäº¤äº’å¼ç¯å¢ƒï¼Œè‡ªåŠ¨ä½¿ç”¨
            print("(éäº¤äº’å¼ç¯å¢ƒï¼Œè‡ªåŠ¨ä½¿ç”¨æ£€æµ‹åˆ°çš„æ—¥æœŸ)")
            return auto_date
    
    # æ‰‹åŠ¨è¾“å…¥
    while True:
        try:
            date_input = input("\nè¯·è¾“å…¥èµ·å§‹æ—¥æœŸ (æ ¼å¼ï¼š2025-10-08): ").strip()
            if re.match(r'\d{4}-\d{2}-\d{2}', date_input):
                return date_input
            print("âŒ æ ¼å¼é”™è¯¯ï¼Œè¯·é‡æ–°è¾“å…¥")
        except EOFError:
            print("\nâŒ æ— æ³•è·å–è¾“å…¥ï¼Œè¯·ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼špython bot.py 2025-10-08")
            sys.exit(1)

def save_full_data(output_file, repos_with_data):
    """ä¿å­˜æ‰€æœ‰ä»“åº“çš„å®Œæ•´æ•°æ®ï¼Œä¾›åç»­ç”Ÿæˆå†™ä½œæŒ‡å¼•ä½¿ç”¨"""
    full_data = {}
    for repo, data in repos_with_data:
        full_data[repo['url']] = {
            'repo': repo,
            'full': data
        }
    
    json_file = str(output_file).replace('.md', '_full_data.json')
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(full_data, f, ensure_ascii=False, indent=2)
    
    return json_file

# ============ ä¸»æµç¨‹ ============

def main():
    print("ğŸ¤– MoonBit å‘¨æŠ¥ä»“åº“æ”¶é›†å™¨ v3.0 (DeepSeek-V3)")
    print("â”€" * 60)
    
    # è®¾ç½®æ—¥å¿—
    setup_logging()
    
    # è·å–æ—¥æœŸ
    since_date = get_search_date()
    print(f"ğŸ“… æœç´¢æ—¥æœŸï¼š{since_date}")
    
    # åˆå§‹åŒ–
    fetcher = GitHubFetcher()
    classifier = RepoClassifier()
    formatter = MarkdownFormatter()
    
    # æ­¥éª¤ 1: æœç´¢ä»“åº“
    print(f"\nğŸ” æœç´¢...", end=' ', flush=True)
    repos = fetcher.search_repos(since_date)
    print(f"æ‰¾åˆ° {len(repos)} ä¸ªä»“åº“")
    
    if not repos:
        print("âœ… æ²¡æœ‰æ–°ä»“åº“ï¼Œé€€å‡º")
        return
    
    # æ­¥éª¤ 2: æŠ“å–å®Œæ•´æ•°æ®
    print(f"\nğŸ“¥ æŠ“å–å®Œæ•´æ•°æ®...", end=' ', flush=True)
    repos_with_data = []
    fetch_success = 0
    fetch_failed = 0
    
    for idx, repo in enumerate(repos, 1):
        full_data = fetcher.fetch_full_repo_data(repo['url'])
        
        if full_data:
            repo['full_data'] = full_data
            
            # è¯†åˆ«ä½œè€…
            author_info = fetcher.identify_author(repo, full_data)
            repo['author_info'] = author_info
            
            repos_with_data.append((repo, full_data))
            fetch_success += 1
        else:
            repo['review_reason'] = 'æ•°æ®æŠ“å–å¤±è´¥'
            repos_with_data.append((repo, None))
            fetch_failed += 1
    
    print(f"{fetch_success}/{len(repos)} æˆåŠŸ")
    
    # æ­¥éª¤ 3: AI åˆ†ç±»
    print(f"\nğŸ¤– AI åˆ†ç±»...", end=' ', flush=True)
    classified = classifier.classify_repos(repos_with_data)
    
    # å¤„ç†éœ€è¦ Review çš„ä»“åº“ï¼ˆç»„ç»‡/Fork å¿…é¡» Reviewï¼‰
    for repo, _ in repos_with_data:
        author_info = repo.get('author_info', {})
        
        # ç»„ç»‡ä»“åº“æˆ– Fork ä»“åº“å¿…é¡»è¿›å…¥ Review åŒº
        if author_info.get('type') in ['organization', 'fork']:
            if repo not in classified['review']:
                repo['review_reason'] = author_info.get('review_reason', 'éœ€è¦äººå·¥ç¡®è®¤')
                classified['review'].append(repo)
                
                # ä» project/package ä¸­ç§»é™¤
                for category in ['project', 'package']:
                    if repo in classified.get(category, []):
                        classified[category].remove(repo)
    
    # æ˜¾ç¤ºåˆ†ç±»ç»“æœ
    print(f"ğŸ“¦ {len(classified['package'])} Package | ğŸš€ {len(classified['project'])} Project | âš ï¸  {len(classified['review'])} Review")
    
    # æ­¥éª¤ 4: ç”Ÿæˆè¾“å‡º
    weekly_num = get_weekly_number()
    date_str = datetime.now().strftime('%Y-%m-%d')
    
    output_content = formatter.format_output(classified, since_date, weekly_num)
    
    # è¦†ç›–é˜²æŠ¤ï¼šè‹¥æ–‡ä»¶å­˜åœ¨ä¸”å«æœ‰ review ç­¾åï¼Œé™¤é--forceï¼Œå¦åˆ™æ‹’ç»è¦†ç›–
    output_filename = OUTPUT_FILENAME_FORMAT.format(
        weekly_num=weekly_num,
        date=date_str
    )
    output_file = OUTPUT_DIR / output_filename
    if output_file.exists():
        try:
            existing = output_file.read_text(encoding='utf-8')
            if 'weekly_bot_reviewed' in existing and '--force' not in sys.argv:
                print(f"\nâš ï¸  æ–‡ä»¶å·²è¢« reviewï¼Œé˜»æ­¢è¦†ç›–ï¼š{output_file.name}")
                print(f"å¦‚éœ€è¦†ç›–è¯·æ·»åŠ  --force å‚æ•°")
                return
        except Exception:
            pass
    output_file.write_text(output_content, encoding='utf-8')
    
    # ä¿å­˜å®Œæ•´æ•°æ® JSON
    json_file = save_full_data(output_file, repos_with_data)
    
    # æ˜¾ç¤ºæˆæœ¬
    ai_stats = classifier.get_usage_stats()
    print(f"ğŸ’° æˆæœ¬ï¼šÂ¥{ai_stats['estimated_cost']:.4f}")
    
    # ä¿å­˜ä¿¡æ¯
    print(f"\nğŸ“„ è¾“å‡ºï¼š{output_file.name}")
    
    # æ ¹æ®æ˜¯å¦æœ‰ Review ä»“åº“ï¼Œç»™å‡ºä¸åŒçš„ä¸‹ä¸€æ­¥æŒ‡å¼•
    review_count = len(classified['review'])
    
    if review_count > 0:
        print(f"\nâš ï¸  å‘ç° {review_count} ä¸ªå¾… Review ä»“åº“")
        print("\nğŸ‘‰ æŒ‰å›è½¦è¿›å…¥äº¤äº’å¼ Reviewï¼Œæˆ– Ctrl+C é€€å‡º")
        try:
            input()
        except (EOFError, KeyboardInterrupt):
            print("\nå·²é€€å‡º")
            return
        
        # è‡ªåŠ¨è°ƒç”¨ review.pyï¼ˆå¤±è´¥åˆ™ä¸­æ­¢ï¼‰
        result = subprocess.run(['python', 'review.py', str(output_file)])
        if result.returncode != 0:
            print("\nâŒ Review å¤±è´¥ï¼Œå·²åœæ­¢ã€‚è¯·ä¿®å¤åé‡è¯•ï¼š")
            print(f"python review.py {output_file}")
            return
        
        # Review å®Œæˆåï¼Œæç¤ºç”Ÿæˆå†™ä½œæŒ‡å¼•
        print("\nğŸ‘‰ æŒ‰å›è½¦ç”Ÿæˆå†™ä½œæŒ‡å¼•ï¼Œæˆ– Ctrl+C é€€å‡º")
        try:
            input()
        except (EOFError, KeyboardInterrupt):
            print("\nå·²é€€å‡º")
            return
        
        result = subprocess.run(['python', 'generate_writing_guide.py', str(output_file)])
        if result.returncode != 0:
            print("\nâŒ å†™ä½œæŒ‡å¼•ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
            return
        
        # æç¤ºè¿è¡Œ postcheck
        print(f"\nğŸ“‹ å»ºè®®è¿è¡Œå‘å¸ƒå‰æ£€æŸ¥ï¼š")
        print(f"python tools/weekly_bot/postcheck.py trees/weekly/weekly{get_weekly_number()}")
    else:
        print("\nğŸ‘‰ æŒ‰å›è½¦ç”Ÿæˆå†™ä½œæŒ‡å¼•ï¼Œæˆ– Ctrl+C é€€å‡º")
        try:
            input()
        except (EOFError, KeyboardInterrupt):
            print("\nå·²é€€å‡º")
            return
        
        result = subprocess.run(['python', 'generate_writing_guide.py', str(output_file)])
        if result.returncode != 0:
            print("\nâŒ å†™ä½œæŒ‡å¼•ç”Ÿæˆå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
            return
        
        # æç¤ºè¿è¡Œ postcheck
        print(f"\nğŸ“‹ å»ºè®®è¿è¡Œå‘å¸ƒå‰æ£€æŸ¥ï¼š")
        print(f"python tools/weekly_bot/postcheck.py trees/weekly/weekly{get_weekly_number()}")
    
    print("\nâœ… å‘¨æŠ¥è‡ªåŠ¨åŒ–å®Œæˆï¼")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­")
        sys.exit(0)
    except Exception as e:
        logging.error(f"è¿è¡Œé”™è¯¯ï¼š{e}", exc_info=True)
        print(f"\nâŒ é”™è¯¯ï¼š{e}")
        sys.exit(1)

