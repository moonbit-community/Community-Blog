#!/usr/bin/env python3
"""
å‘¨æŠ¥URLæ”¶é›†å™¨
åŠŸèƒ½ï¼šæ”¶é›†ä¸Šæ¬¡å‘¨æŠ¥ä¹‹åæ–°åˆ›å»ºçš„MoonBitä»“åº“URL
"""

import requests
import os
import re
from datetime import datetime
from typing import List, Dict, Optional

# è·¯å¾„å¸¸é‡ï¼šä»¥å½“å‰æ–‡ä»¶ä¸ºåŸºå‡†ï¼Œå®šä½é¡¹ç›®æ ¹ä¸è¾“å‡ºç›®å½•
BASE_DIR = os.path.dirname(os.path.abspath(__file__))  # tools/weekly_bot
PROJECT_ROOT = os.path.dirname(os.path.dirname(BASE_DIR))  # é¡¹ç›®æ ¹
WEEKLY_DIR = os.path.join(PROJECT_ROOT, "trees", "weekly")  # å‘¨æŠ¥ç›®å½•
OUTPUT_DIR = os.path.join(BASE_DIR, "output")  # è¾“å‡ºç›®å½•
os.makedirs(OUTPUT_DIR, exist_ok=True)

class WeeklyURLCollector:
    """å‘¨æŠ¥URLæ”¶é›†å™¨"""
    
    def __init__(self, github_token: Optional[str] = None):
        """åˆå§‹åŒ–"""
        self.github_token = github_token or os.getenv('GITHUB_TOKEN')
        self.headers = {
            'Accept': 'application/vnd.github.v3+json',
            'User-Agent': 'Weekly-URL-Collector/1.0'
        }
        if self.github_token:
            self.headers['Authorization'] = f'token {self.github_token}'
    
    def get_last_weekly_date(self) -> datetime:
        """ä»æœ€æ–°å‘¨æŠ¥æ–‡ä»¶æå–ç»“æŸæ—¶é—´"""
        weekly_dir = WEEKLY_DIR
        
        # è·å–æ‰€æœ‰å‘¨æŠ¥æ–‡ä»¶
        weekly_files = []
        for filename in os.listdir(weekly_dir):
            if filename.startswith('weekly') and filename.endswith('.md') and filename != 'index.md':
                weekly_files.append(filename)
        
        if not weekly_files:
            print("âš ï¸  æœªæ‰¾åˆ°å‘¨æŠ¥æ–‡ä»¶ï¼Œä½¿ç”¨30å¤©å‰ä½œä¸ºé»˜è®¤æ—¶é—´")
            return datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
        
        # æŒ‰æ•°å­—æ’åºï¼Œè·å–æœ€æ–°çš„å‘¨æŠ¥
        def extract_number(filename):
            match = re.search(r'weekly(\d+)', filename)
            return int(match.group(1)) if match else 0
        
        latest_weekly = sorted(weekly_files, key=extract_number)[-1]
        latest_path = os.path.join(weekly_dir, latest_weekly)
        
        print(f"ğŸ“„ è¯»å–æœ€æ–°å‘¨æŠ¥: {latest_weekly}")
        
        with open(latest_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æå–æ—¶é—´èŒƒå›´ï¼š2025/9/22 ~ 2025/10/8
        time_pattern = r'(\d{4}/\d{1,2}/\d{1,2})\s*~\s*(\d{4}/\d{1,2}/\d{1,2})'
        match = re.search(time_pattern, content)
        
        if match:
            end_date_str = match.group(2)  # è·å–ç»“æŸæ—¶é—´
            end_date = datetime.strptime(end_date_str, '%Y/%m/%d')
            print(f"â° ä¸Šæ¬¡å‘¨æŠ¥ç»“æŸæ—¶é—´: {end_date.date()}")
            return end_date
        else:
            print("âš ï¸  æ— æ³•æå–æ—¶é—´ï¼Œä½¿ç”¨7å¤©å‰ä½œä¸ºé»˜è®¤æ—¶é—´")
            return datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
    
    def search_with_update_filter(self, query: str, since_date: datetime) -> List[Dict]:
        """æœç´¢å¹¶æŒ‰æ›´æ–°æ—¶é—´è¿‡æ»¤"""
        print(f"\nğŸ” æœç´¢: {query}")
        results = []
        page = 1
        max_pages = 10  # æœ€å¤š10é¡µ
        
        while page <= max_pages:
            url = "https://api.github.com/search/repositories"
            params = {
                'q': query,
                'sort': 'updated',
                'order': 'desc',
                'per_page': 100,
                'page': page
            }
            
            try:
                response = requests.get(url, headers=self.headers, params=params)
                response.raise_for_status()
                
                data = response.json()
                items = data.get('items', [])
                
                if not items:
                    break
                
                # æ£€æŸ¥æ¯ä¸ªä»“åº“çš„æ›´æ–°æ—¶é—´
                for repo in items:
                    updated_at = datetime.fromisoformat(
                        repo['updated_at'].replace('Z', '+00:00')
                    ).date()
                    
                    # å¦‚æœæ›´æ–°æ—¶é—´æ—©äºæˆ–ç­‰äºå‘¨æŠ¥æ—¶é—´ï¼Œåœæ­¢æœç´¢
                    if updated_at <= since_date.date():
                        print(f"   â¹ï¸  é‡åˆ° updated_at <= {since_date.date()}ï¼Œåœæ­¢æœç´¢")
                        return results
                    
                    results.append(repo)
                
                print(f"   ğŸ“„ ç¬¬{page}é¡µ: {len(items)}ä¸ªä»“åº“")
                page += 1
                
            except requests.exceptions.RequestException as e:
                print(f"   âŒ APIè¯·æ±‚å¤±è´¥: {e}")
                break
        
        print(f"   âœ… ç¬¬ä¸€æ¬¡è¿‡æ»¤å®Œæˆ: {len(results)}ä¸ªä»“åº“")
        return results
    
    def deduplicate(self, repos: List[Dict]) -> List[Dict]:
        """æŒ‰ä»“åº“IDå»é‡"""
        seen = set()
        unique_repos = []
        
        for repo in repos:
            repo_id = repo['id']
            if repo_id not in seen:
                seen.add(repo_id)
                unique_repos.append(repo)
        
        return unique_repos
    
    def filter_by_created_date(self, repos: List[Dict], since_date: datetime) -> List[Dict]:
        """æŒ‰åˆ›å»ºæ—¶é—´è¿‡æ»¤ï¼ˆç¬¬äºŒæ¬¡è¿‡æ»¤ï¼‰"""
        print(f"\nğŸ” ç¬¬äºŒæ¬¡è¿‡æ»¤: created_at > {since_date.date()}")
        filtered = []
        
        for repo in repos:
            created_at = datetime.fromisoformat(
                repo['created_at'].replace('Z', '+00:00')
            ).date()
            
            # åªä¿ç•™åˆ›å»ºæ—¶é—´æ™šäºå‘¨æŠ¥æ—¶é—´çš„
            if created_at > since_date.date():
                filtered.append(repo)
        
        print(f"   âœ… ç¬¬äºŒæ¬¡è¿‡æ»¤å®Œæˆ: {len(filtered)}ä¸ªä»“åº“")
        return filtered
    
    
    def save_results(self, repos: List[Dict]):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜æ‰€æœ‰URLåˆ°å•ä¸ªæ–‡ä»¶
        urls_filename = os.path.join(OUTPUT_DIR, f"new_repos_urls_{timestamp}.txt")
        with open(urls_filename, 'w', encoding='utf-8') as f:
            for repo in repos:
                f.write(f"{repo['html_url']}\n")
        print(f"\nğŸ“ æ‰€æœ‰ä»“åº“URLå·²ä¿å­˜: {urls_filename}")
        
        # ä¿å­˜è¯¦ç»†ä¿¡æ¯
        detail_filename = os.path.join(OUTPUT_DIR, f"new_repos_detail_{timestamp}.md")
        with open(detail_filename, 'w', encoding='utf-8') as f:
            f.write(f"# æ–°ä»“åº“æ”¶é›†ç»“æœ\n\n")
            f.write(f"æ”¶é›†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ€»è®¡: {len(repos)}ä¸ªæ–°ä»“åº“\n\n")
            
            for i, repo in enumerate(repos, 1):
                f.write(f"{i}. **{repo['full_name']}**\n")
                f.write(f"   - URL: {repo['html_url']}\n")
                f.write(f"   - æè¿°: {repo.get('description', 'æ— ')}\n")
                f.write(f"   - è¯­è¨€: {repo.get('language', 'æœªçŸ¥')}\n")
                f.write(f"   - åˆ›å»º: {repo['created_at'][:10]}\n")
                f.write(f"   - Stars: {repo['stargazers_count']}, Forks: {repo['forks_count']}\n\n")
        
        print(f"ğŸ“ è¯¦ç»†ä¿¡æ¯å·²ä¿å­˜: {detail_filename}")
    
    def run(self):
        """ä¸»æµç¨‹"""
        print("=" * 60)
        print("å‘¨æŠ¥URLæ”¶é›†å™¨")
        print("=" * 60)
        
        # 1. è·å–ä¸Šæ¬¡å‘¨æŠ¥æ—¶é—´
        since_date = self.get_last_weekly_date()
        
        # 2. ç¬¬ä¸€æ¬¡è¿‡æ»¤ï¼šæœç´¢"moonbit"
        repos1 = self.search_with_update_filter("moonbit", since_date)
        
        # 3. ç¬¬ä¸€æ¬¡è¿‡æ»¤ï¼šæœç´¢"language:moonbit"
        repos2 = self.search_with_update_filter("language:moonbit", since_date)
        
        # 4. åˆå¹¶å»é‡
        print(f"\nğŸ”„ åˆå¹¶å»é‡...")
        all_repos = self.deduplicate(repos1 + repos2)
        print(f"   âœ… åˆå¹¶å: {len(all_repos)}ä¸ªä»“åº“")
        
        # 5. ç¬¬äºŒæ¬¡è¿‡æ»¤ï¼šæŒ‰åˆ›å»ºæ—¶é—´
        new_repos = self.filter_by_created_date(all_repos, since_date)
        
        # 6. ä¿å­˜ç»“æœ
        self.save_results(new_repos)
        
        print("\n" + "=" * 60)
        print("âœ… æ”¶é›†å®Œæˆï¼")
        print("=" * 60)
        print(f"\nğŸ“Š ç»Ÿè®¡:")
        print(f"   æ–°ä»“åº“æ€»è®¡: {len(new_repos)}ä¸ª")
        print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print(f"   1. æ‰“å¼€ç”Ÿæˆçš„ new_repos_urls_*.txt æ–‡ä»¶")
        print(f"   2. åœ¨Cursor Chatä¸­è®©AIåˆ†ç±»ä¸º'é¡¹ç›®'å’Œ'åŒ…'")
        print(f"   3. åˆ†åˆ«è®©AIç”Ÿæˆé¡¹ç›®å’ŒåŒ…çš„å‘¨æŠ¥æ¡ç›®")
        print(f"\nğŸ“‹ Cursoråˆ†ç±»æç¤ºè¯:")
        print(f"   'è¯·å°†ä»¥ä¸‹ä»“åº“åˆ†ç±»ä¸º'é¡¹ç›®'å’Œ'åŒ…'ä¸¤ç±»ã€‚")
        print(f"    åˆ†ç±»æ ‡å‡†ï¼š")
        print(f"    - é¡¹ç›®ï¼šå®Œæ•´åº”ç”¨ã€æ¸¸æˆã€å·¥å…·ã€æ¡†æ¶å®ç°ã€æ¼”ç¤º")
        print(f"    - åŒ…ï¼šåº“ã€å·¥å…·åŒ…ã€å¯è¢«å…¶ä»–é¡¹ç›®å¼•ç”¨çš„æ¨¡å—'")
        print(f"\n   ç„¶ååˆ†åˆ«ç”Ÿæˆå‘¨æŠ¥æ¡ç›®ã€‚")

def main():
    """ä¸»å‡½æ•°"""
    github_token = os.getenv('GITHUB_TOKEN')
    if not github_token:
        print("âš ï¸  æœªè®¾ç½®GITHUB_TOKENï¼ŒAPIè°ƒç”¨å¯èƒ½å—é™")
        print("   å»ºè®®: export GITHUB_TOKEN=your_token\n")
    
    collector = WeeklyURLCollector(github_token)
    collector.run()

if __name__ == "__main__":
    main()

